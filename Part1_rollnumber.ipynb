{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ998sgcaAlB"
      },
      "source": [
        "# PA4.1 GANs and Latent space interpolation (35 marks)\n",
        "\n",
        " This part of the assignment involves implementing a Generative Adversarial Network (GAN) to work with the MNIST dataset, which is readily available in the torchvision package. You'll be tasked with training your GAN to generate images that resemble the handwritten digits from MNIST.\n",
        "\n",
        "After your GAN is trained and able to generate images, the next step is to explore latent space interpolation between two images. This means you'll learn how to transition smoothly from one digit to another by navigating through the GAN's latent space, a technique that demonstrates the model's ability to understand and manipulate the underlying features of the digits.\n",
        "\n",
        "Once thats done you will be asked to generate a few images of your own, and then do some mathematical operations with them in the latent space and see what sort of output you can come up with. sounds simple enough right?\n",
        "\n",
        "[All code blocks should already have been run and the outputs should be visible in order to be graded]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Roll number: 25100261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqx5Zdf8aAlC"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do not include extra imports, you should be able to complete this assignment with the following imports only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t83_7r64W7eA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5, std=0.5\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q-yPIspaAlE"
      },
      "source": [
        "# MNIST dataset loading (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets start by actually loading the mnist dataset and visualizing some of the images, dont foget to transform the images to the right size and tensor :) and normalize them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg04dFSwW_kh",
        "outputId": "4afb2240-8fa9-4659-b5c8-656894777ab4"
      },
      "outputs": [],
      "source": [
        "#make sue of the Dataloaeer to load the data\n",
        "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "batch_size = 64  # You can adjust the batch size as needed\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAACxCAYAAADwMnaUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc70lEQVR4nO3dfVTUVf4H8DeIDPjAECiDhCilha3PqDRp2SrrQ4aamGa2knrWows+0aaypa6tiQ9tmuZTe1xtd0OTPaJpqSEqri2iomyZim7rKoqDWfIgyoDM/f2x6/y6X3BkmIG5wPt1zpzTe+b7/fLhgvQ537lzr5sQQoCIiIhIAe6uLoCIiIjoPjYmREREpAw2JkRERKQMNiZERESkDDYmREREpAw2JkRERKQMNiZERESkDDYmREREpAw2JkRERKQMNiZERESkjFprTNauXYv27dvDy8sLEREROH78eG19KSIiImog3Gpjr5xPP/0UEyZMwIYNGxAREYFVq1YhOTkZOTk5CAgIsHmuxWJBXl4eWrZsCTc3N2eXRkRERLVACIHi4mIEBQXB3b3m9z1qpTGJiIhA79698eGHHwL4b7PRtm1bTJ8+HfPmzbN57tWrV9G2bVtnl0RERER1IDc3F8HBwTU+38OJtQAAysrKkJWVhYSEBOtz7u7uiIyMREZGRqXjzWYzzGazNd/vk2bPng2dTufs8oiIiKgWmM1mrFy5Ei1btnToOk5vTG7evImKigoYDAbpeYPBgPPnz1c6PjExEYsWLar0vE6nY2NCRERUzzg6DcPln8pJSEhAYWGh9ZGbm+vqkoiIiMhFnH7HpFWrVmjSpAny8/Ol5/Pz8xEYGFjpeN4ZISIiovucfsfE09MT4eHhSEtLsz5nsViQlpYGo9Ho7C9HREREDYjT75gAQHx8PGJiYtCrVy/06dMHq1atQklJCSZOnFgbX46IiIgaiFppTMaOHYvvv/8eCxYsgMlkQvfu3bFv375KE2JrqqrJslT/LFy40Obr/Dk3DPw5Nw78OTcOD/s5O0OtNCYAEBcXh7i4uNq6PBERETVALv9UDhEREdF9bEyIiIhIGWxMiIiISBlsTIiIiEgZbEyIiIhIGWxMiIiISBlsTIiIiEgZbEyIiIhIGWxMiIiISBlsTIiIiEgZbEyIiIhIGWxMiIiISBlsTIiIiEgZbEyIiIhIGWxMiIiISBkeri6AiKiu7dy5U8pXrlyR8owZM+qwmsrGjh0r5YkTJ0p54cKFUs7MzKz1mojqCu+YEBERkTLYmBAREZEy2JgQERGRMjjHhIgaHSGElIcPHy5lV88xKS8vl3JkZKSUw8PDpdy9e3cpX7t2rVbqIqoLvGNCREREymBjQkRERMpgY0JERETK4BwTJ9m2bZuUo6KipDxixAgpHzhwoNZrotq3ePFiKWvXn+jQoYOUt2/fLuU33nhDylevXnVidVRf7dixQ8r79u2T8gsvvCDlJ554QsqcY+Iabm5uUm7atKmUn3vuOSk/+eSTUjYYDFKOjY2Vsp+fn5RDQkKknJubW/1iFcY7JkRERKQMNiZERESkDDYmREREpAzOMamhrl27Sjk6OlrKTZo0kXJKSoqUw8LCpMz3hNXk7e0t5U2bNkn5pZdekrKnp6eULRaLlEePHi3l/v37S3natGlS1v7eEAFAcXGxlPn3wzl0Op2UtXNExowZI2Xt3J62bdtK+ZVXXnFidZX/nvznP/+RcqtWrSqdc+vWLafWUBd4x4SIiIiUwcaEiIiIlGF3Y3LkyBFERUUhKCgIbm5ulbYPF0JgwYIFaNOmDby9vREZGYmLFy86q14iIiJqwOyeY1JSUoJu3bph0qRJGDVqVKXXly9fjtWrV+Pjjz9GaGgo5s+fj8GDB+Ps2bPw8vJyStEq0H7v2vf+5s+fL+UlS5ZIedy4cVJ+7733nFgdOYt2PYlBgwY59fqtW7eWcnJyspTPnDkjZe3v0d///ncpX79+3YnVkapatmwp5UcffVTKFy5cqMty6g0fHx8pL1q0SMraf9/auYCq0a6HdefOHRdV4lx2NyZDhw7F0KFDq3xNCIFVq1bh7bffti4o9uc//xkGgwE7d+50+kQgIiIialicOsfk0qVLMJlM0k6Yer0eERERyMjIqPIcs9mMoqIi6UFERESNk1MbE5PJBKDysroGg8H6mlZiYiL0er31of24FRERETUeLl/HJCEhAfHx8dZcVFSkZHOi3ZNgwYIFUv7LX/4i5aVLl0pZ+z0lJiZKOSsrS8qHDh2qUZ3kXOHh4XYdf/PmTSlr15fo1q2bzfO1e2106dJFylu3brX59TZs2CDlhQsX2vx6jUGzZs0qPdepUycpl5SU1FU51RIQECDlgQMHuqiShkX7uzBjxgyHrqddI6S0tNTm8dp1RrTrpDzMP/7xDymPHDlSymaz2a7rqcqpd0wCAwMBAPn5+dLz+fn51te0dDodfHx8pAcRERE1Tk5tTEJDQxEYGIi0tDTrc0VFRcjMzITRaHTmlyIiIqIGyO63cm7fvo1//etf1nzp0iVkZ2fDz88PISEhmDVrFhYvXoyOHTtaPy4cFBRU6ZYTERERkZbdjcnJkyfx85//3Jrvzw+JiYnBli1bMGfOHJSUlGDKlCkoKChAv379sG/fvnq/hon2vbsvvvhCym+++abN87Wvd+jQQcp79+6Vsna9Cu3r2vUt2rdvL+Vz587ZrIeqZ926dVKeM2eOlLW/B5MmTZJyWVmZlENDQ6X81ltvSXn48OFSbt68uc36tO9ZT506VcqcY1L1HBPtHieqjZN2ry3tHi7aOTHat8+paoWFhVLW/l2Oi4uT8vfffy/ljz76SMr79++X8tWrV6Ws/f/eV199JeXu3bvbrFf7c9bOXbx7967N8+sruxuT559/HkKIB77u5uaGd955B++8845DhREREVHjw71yiIiISBlsTIiIiEgZLl/HpL7Qvof74osv2nW+dg8D7WTguXPnSnns2LFS1s5F0F7vxIkTUnb2ni6NlXa9Ge2ckuPHj9t1Pe3cn9dee03KnTt3lvKyZcukPGTIELu+Hv13/ltDo5034+fn56JK6hftnIz333/fZraXt7e3lNesWSPlh80p0Xr99del/Pnnn9ekrHqHd0yIiIhIGWxMiIiISBlsTIiIiEgZnGPiItr3On/3u9/ZzNol/bV781Dt0K5fY++cEntp16fRrq8xYMAAKXt6etZqPQ1BdcbowoULdVCJ82j3VLJ3zxWqHcOGDZPyxIkT7Tp/165dUv7yyy8drqk+4h0TIiIiUgYbEyIiIlIGGxMiIiJSBueY1BMmk0nK2r0zSktL67IcqiPavW8eNl8iLy+vNsupl/z9/R96THp6eh1UUn3VqZlcT7uezBtvvGHX+ZmZmVLWrrlz+/btmhVWz/GOCRERESmDjQkREREpg40JERERKYNzTOoJHx8fKRuNRimnpqbWZTlURzp06GDX8UuWLKmlSuqP4OBgKU+YMKHSMUePHpXyzZs3a7WmoKAgKb/99ttSjoiIkHKbNm3suv7GjRulPGbMmErHZGdn23VNejjtv7c+ffrYPL6goEDKixcvlnJxcbFT6qrveMeEiIiIlMHGhIiIiJTBxoSIiIiUwcaEiIiIlMHJr/WEdtOuJk2aSHnfvn11WQ7VEu2kxyeeeMLm8doJjXv27HF2ScrTLjqnXSytVatWlc7ZvXu3lO/du2fX1+zatauUH3vsMSmPHDlSyv369bN5vHZSpHZTQe0mnlraSdJV/d5w8qv9tAuoRUVFSfnVV1+163qxsbFS/uKLL2pWWAPHOyZERESkDDYmREREpAw2JkRERKQMzjFxkWeffVbKHh62fxQPe3/44MGDjpZECnjvvfekbDAYbB7/5ZdfSvnu3btOr0l1PXr0kHJoaKiUqxqTw4cPS1mv10v5hRdekHJCQoKUH3/8cSl7e3vbrPHSpUtSnjhxopTT0tKkrF1o6+uvv5ayr6+vlEePHi3lAwcO2KyHqkf7u5SUlGTz+Fu3bkk5OTlZyp9//rlzCmvgeMeEiIiIlMHGhIiIiJTBxoSIiIiUwTkmdUS7uZP2PW7tOiVaD5s7kJiYKOXbt29L+dixY1L+4IMPbF6P6oZ2rsDLL79s8/jvvvtOyuvXr3d2SfWOdtM+Le2aPwCQkpIiZe0me/bSbqKpvf7evXulfPnyZZvX8/LykrJ2k8G2bdtKWbsOixDC5vWpat26dZPy3Llz7Tp/x44dUp42bZrDNTVGvGNCREREyrCrMUlMTETv3r3RsmVLBAQEYOTIkcjJyZGOKS0tRWxsLPz9/dGiRQtER0cjPz/fqUUTERFRw2RXY5Keno7Y2FgcO3YMqampKC8vx6BBg1BSUmI9Zvbs2di9ezeSk5ORnp6OvLw8jBo1yumFExERUcNj1xwT7X4sW7ZsQUBAALKysvDcc8+hsLAQmzZtQlJSEgYMGAAA2Lx5Mzp16oRjx47h6aefdl7l9czx48el3L9/fym/+OKLNs+Pj4+XcllZmZS16yRcvHhRyp999lm16qTa5efnJ+Vdu3ZJuar5ED+1adMmKV+5csU5hdVj2v1GSktLpaxdEwSoPKfr+vXrUtbOCbl27ZqUtXvtnDx5snrFVpP2e9DOLerZs6eUtXu6UM1o/86OHTvWrvO3b9/uzHIaLYfmmBQWFgL4/z+2WVlZKC8vR2RkpPWYsLAwhISEICMjw5EvRURERI1AjT+VY7FYMGvWLPTt2xedO3cGAJhMJnh6elb6pIHBYIDJZKryOmazGWaz2ZqLiopqWhIRERHVczW+YxIbG4szZ85g27ZtDhWQmJgIvV5vfWg/BkdERESNR43umMTFxWHPnj04cuSItIZAYGAgysrKUFBQIN01yc/PR2BgYJXXSkhIkN7XKyoqahTNydGjR21mrbi4OCn/5je/kfKGDRucUxjVKu06Cc8884xd52vnmFDlNX60Y6r95GBV56hu+vTpUh48eLCUN27cKOX7c/x+6sKFC84vrJ5r06aNlKOiouw6f+vWrVLmlAXnsOuOiRACcXFxSElJwcGDByttcBQeHo6mTZtKG1Ll5OTgypUrMBqNVV5Tp9PBx8dHehAREVHjZNcdk9jYWCQlJWHXrl1o2bKldd6IXq+Ht7c39Ho9Jk+ejPj4ePj5+cHHxwfTp0+H0Whs1J/IISIiouqxqzG5v/z1888/Lz2/efNmvP766wCAlStXwt3dHdHR0TCbzRg8eDDWrVvnlGKJiIioYbOrManO/gteXl5Yu3Yt1q5dW+OiqHLzp5WcnFw3hZBDtOtlJCQk2HX+xx9/LOWCggJHS2rwsrOzXV2C02lXz/7Tn/4k5ZkzZ0p5yJAhla7BOSZA69atpfzVV19JWa/X2zxf+2GPKVOmSPnOnTsOVEf3ca8cIiIiUgYbEyIiIlIGGxMiIiJSRo1XfqXa9eyzz0r53LlzUv7hhx/qshyqocmTJ0t54MCBNo+/efOmlBctWiTle/fuOacwqte0e+lQ9fTu3VvK7dq1s3n8rVu3pLxixQopc05J7eAdEyIiIlIGGxMiIiJSBhsTIiIiUgbnmChK+3n6v/3tby6qhOyh3Vn71Vdftev85cuXS/ny5cuOlkTUKGn/LQLA9u3b7bpGXl6elBviGjkq4h0TIiIiUgYbEyIiIlIGGxMiIiJSBueYKCoqKkrKPXv2dFElZI+lS5dKuX///jaP165LsmrVKmeXRI3Ad999J2XOSQPmzJlT6Tlvb2+b52j3otLuhUN1g3dMiIiISBlsTIiIiEgZbEyIiIhIGZxjooiQkBApWywWKZeVldVlOVRN2vesBw0aZNf52nVLKioqHK6JGr6EhASbmR4+n6Qqu3btkvKxY8ecVQ7ZgXdMiIiISBlsTIiIiEgZbEyIiIhIGWxMiIiISBmc/KoI7UJbFy9elHJ5eXldlkMP0LZtWykvWLBAyu3atbN5fnJyspSXLFninMKIyG579uyRMhc4VAPvmBAREZEy2JgQERGRMtiYEBERkTI4x0QReXl5Uh4+fLiLKiFbcnNzpfyrX/3KZiYi15g9e3a1niP18I4JERERKYONCRERESmDjQkREREpg40JERERKYONCRERESnDrsZk/fr16Nq1K3x8fODj4wOj0Yi9e/daXy8tLUVsbCz8/f3RokULREdHIz8/3+lFExERUcNkV2MSHByMpUuXIisrCydPnsSAAQMwYsQIfPvttwD++1Gs3bt3Izk5Genp6cjLy8OoUaNqpXAiIiJqeNyEEMKRC/j5+WHFihUYPXo0WrdujaSkJIwePRoAcP78eXTq1AkZGRl4+umnq3W9oqIi6PV6zJs3DzqdzpHSiIiIqI6YzWYsXboUhYWF8PHxqfF1ajzHpKKiAtu2bUNJSQmMRiOysrJQXl6OyMhI6zFhYWEICQlBRkbGA69jNptRVFQkPYiIiKhxsrsx+eabb9CiRQvodDpMnToVKSkpeOqpp2AymeDp6QlfX1/peIPBAJPJ9MDrJSYmQq/XWx/a3VuJiIio8bC7MXnyySeRnZ2NzMxMTJs2DTExMTh79myNC0hISEBhYaH1oV3ym4iIiBoPu/fK8fT0RIcOHQAA4eHhOHHiBD744AOMHTsWZWVlKCgokO6a5OfnIzAw8IHX0+l0nEtCREREAJywjonFYoHZbEZ4eDiaNm2KtLQ062s5OTm4cuUKjEajo1+GiIiIGgG77pgkJCRg6NChCAkJQXFxMZKSknD48GHs378fer0ekydPRnx8PPz8/ODj44Pp06fDaDRW+xM5RERE1LjZ1ZjcuHEDEyZMwPXr16HX69G1a1fs378fv/jFLwAAK1euhLu7O6Kjo2E2mzF48GCsW7fOroLuf3rZbDbbdR4RERG5zv3/bzu4Conj65g429WrV/nJHCIionoqNzcXwcHBNT5fucbEYrEgLy8PQgiEhIQgNzfXoYVaGruioiK0bduW4+gAjqHjOIbOwXF0HMfQcQ8aQyEEiouLERQUBHf3mk9htftTObXN3d0dwcHB1oXW7u/LQ47hODqOY+g4jqFzcBwdxzF0XFVjqNfrHb4udxcmIiIiZbAxISIiImUo25jodDosXLiQi685iOPoOI6h4ziGzsFxdBzH0HG1PYbKTX4lIiKixkvZOyZERETU+LAxISIiImWwMSEiIiJlsDEhIiIiZSjbmKxduxbt27eHl5cXIiIicPz4cVeXpKzExET07t0bLVu2REBAAEaOHImcnBzpmNLSUsTGxsLf3x8tWrRAdHQ08vPzXVSx+pYuXQo3NzfMmjXL+hzHsHquXbuG1157Df7+/vD29kaXLl1w8uRJ6+tCCCxYsABt2rSBt7c3IiMjcfHiRRdWrJaKigrMnz8foaGh8Pb2xuOPP47f//730v4jHEPZkSNHEBUVhaCgILi5uWHnzp3S69UZrx9//BHjx4+Hj48PfH19MXnyZNy+fbsOvwvXszWO5eXlmDt3Lrp06YLmzZsjKCgIEyZMQF5ennQNZ4yjko3Jp59+ivj4eCxcuBCnTp1Ct27dMHjwYNy4ccPVpSkpPT0dsbGxOHbsGFJTU1FeXo5BgwahpKTEeszs2bOxe/duJCcnIz09HXl5eRg1apQLq1bXiRMnsHHjRnTt2lV6nmP4cLdu3ULfvn3RtGlT7N27F2fPnsUf/vAHPPLII9Zjli9fjtWrV2PDhg3IzMxE8+bNMXjwYJSWlrqwcnUsW7YM69evx4cffohz585h2bJlWL58OdasWWM9hmMoKykpQbdu3bB27doqX6/OeI0fPx7ffvstUlNTsWfPHhw5cgRTpkypq29BCbbG8c6dOzh16hTmz5+PU6dOYceOHcjJycHw4cOl45wyjkJBffr0EbGxsdZcUVEhgoKCRGJiogurqj9u3LghAIj09HQhhBAFBQWiadOmIjk52XrMuXPnBACRkZHhqjKVVFxcLDp27ChSU1NF//79xcyZM4UQHMPqmjt3rujXr98DX7dYLCIwMFCsWLHC+lxBQYHQ6XRi69atdVGi8oYNGyYmTZokPTdq1Cgxfvx4IQTH8GEAiJSUFGuuznidPXtWABAnTpywHrN3717h5uYmrl27Vme1q0Q7jlU5fvy4ACAuX74shHDeOCp3x6SsrAxZWVmIjIy0Pufu7o7IyEhkZGS4sLL6o7CwEADg5+cHAMjKykJ5ebk0pmFhYQgJCeGYasTGxmLYsGHSWAEcw+r67LPP0KtXL7z88ssICAhAjx498Mc//tH6+qVLl2AymaRx1Ov1iIiI4Dj+zzPPPIO0tDRcuHABAPDPf/4TR48exdChQwFwDO1VnfHKyMiAr68vevXqZT0mMjIS7u7uyMzMrPOa64vCwkK4ubnB19cXgPPGUblN/G7evImKigoYDAbpeYPBgPPnz7uoqvrDYrFg1qxZ6Nu3Lzp37gwAMJlM8PT0tP7y3GcwGGAymVxQpZq2bduGU6dO4cSJE5Ve4xhWz7///W+sX78e8fHx+O1vf4sTJ05gxowZ8PT0RExMjHWsqvr3zXH8r3nz5qGoqAhhYWFo0qQJKioq8O6772L8+PEAwDG0U3XGy2QyISAgQHrdw8MDfn5+HNMHKC0txdy5czFu3DjrRn7OGkflGhNyTGxsLM6cOYOjR4+6upR6JTc3FzNnzkRqaiq8vLxcXU69ZbFY0KtXLyxZsgQA0KNHD5w5cwYbNmxATEyMi6urH7Zv345PPvkESUlJ+NnPfobs7GzMmjULQUFBHENSQnl5OcaMGQMhBNavX+/06yv3Vk6rVq3QpEmTSp92yM/PR2BgoIuqqh/i4uKwZ88eHDp0CMHBwdbnAwMDUVZWhoKCAul4jun/y8rKwo0bN9CzZ094eHjAw8MD6enpWL16NTw8PGAwGDiG1dCmTRs89dRT0nOdOnXClStXAMA6Vvz3/WBvvvkm5s2bh1deeQVdunTBL3/5S8yePRuJiYkAOIb2qs54BQYGVvpwxb179/Djjz9yTDXuNyWXL19Gamqq9W4J4LxxVK4x8fT0RHh4ONLS0qzPWSwWpKWlwWg0urAydQkhEBcXh5SUFBw8eBChoaHS6+Hh4WjatKk0pjk5Obhy5QrH9H8GDhyIb775BtnZ2dZHr169MH78eOt/cwwfrm/fvpU+qn7hwgW0a9cOABAaGorAwEBpHIuKipCZmclx/J87d+7A3V3+09ykSRNYLBYAHEN7VWe8jEYjCgoKkJWVZT3m4MGDsFgsiIiIqPOaVXW/Kbl48SIOHDgAf39/6XWnjWMNJuvWum3btgmdTie2bNkizp49K6ZMmSJ8fX2FyWRydWlKmjZtmtDr9eLw4cPi+vXr1sedO3esx0ydOlWEhISIgwcPipMnTwqj0SiMRqMLq1bfTz+VIwTHsDqOHz8uPDw8xLvvvisuXrwoPvnkE9GsWTPx17/+1XrM0qVLha+vr9i1a5f4+uuvxYgRI0RoaKi4e/euCytXR0xMjHj00UfFnj17xKVLl8SOHTtEq1atxJw5c6zHcAxlxcXF4vTp0+L06dMCgHj//ffF6dOnrZ8Wqc54DRkyRPTo0UNkZmaKo0ePio4dO4px48a56ltyCVvjWFZWJoYPHy6Cg4NFdna29P8as9lsvYYzxlHJxkQIIdasWSNCQkKEp6en6NOnjzh27JirS1IWgCofmzdvth5z9+5d8etf/1o88sgjolmzZuKll14S169fd13R9YC2MeEYVs/u3btF586dhU6nE2FhYeKjjz6SXrdYLGL+/PnCYDAInU4nBg4cKHJyclxUrXqKiorEzJkzRUhIiPDy8hKPPfaYeOutt6Q//hxD2aFDh6r8GxgTEyOEqN54/fDDD2LcuHGiRYsWwsfHR0ycOFEUFxe74LtxHVvjeOnSpQf+v+bQoUPWazhjHN2E+MlygkREREQupNwcEyIiImq82JgQERGRMtiYEBERkTLYmBAREZEy2JgQERGRMtiYEBERkTLYmBAREZEy2JgQERGRMtiYEBERkTLYmBAREZEy2JgQERGRMtiYEBERkTL+D7taphLsEJ0CAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 7 4 7\n"
          ]
        }
      ],
      "source": [
        "import torchvision.utils as utils\n",
        "# Visualize the MNIST Dataset\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(utils.make_grid(images[:4]))\n",
        "# Print labels\n",
        "print(' '.join(f'{labels[j]}' for j in range(4)))\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUj6dfEZaAlF"
      },
      "source": [
        "# Generator and Discriminator (10 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next lets set up the main charecters of our GAN, the generator and the discriminator. Starting of with the generator, feel free to use, whatever sort of architecture you feel suits you, just make sure to use the right activation functions and normalization layers, and that both the generator and the discriminator are able to work with the right size of images.\n",
        "\n",
        "Feel free to reshape the tensor before passing it through the model, u may explicitly need to do this in atleast one of the models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-bO03K8iXFTi"
      },
      "outputs": [],
      "source": [
        "# Code the Generator Model here\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, img_shape),\n",
        "            nn.Tanh()  # Tanh activation function for generating images in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, latent_dim, img_shape):\n",
        "#         super(Generator, self).__init__()\n",
        "\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Linear(latent_dim, 32),\n",
        "#             nn.BatchNorm1d(32),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Linear(32, 64),\n",
        "#             nn.BatchNorm1d(64),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Linear(64, 128),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Linear(128, img_shape),\n",
        "#             # nn.Dropout(0.3),\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UgAMha2cXHQ1"
      },
      "outputs": [],
      "source": [
        "# Code the Discriminator Model here\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(img_shape, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Sigmoid activation function for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self, img_shape):\n",
        "#         super(Discriminator, self).__init__()\n",
        "\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Linear(img_shape, 128),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(0.3),  # Adding dropout for regularization\n",
        "#             nn.Linear(128, 64),\n",
        "#             nn.BatchNorm1d(64),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(0.3),  # Adding dropout for regularization\n",
        "#             nn.Linear(64, 32),\n",
        "#             nn.BatchNorm1d(32),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(0.3),  # Adding dropout for regularization\n",
        "#             nn.Linear(32, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A-nszA7iXMX5"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100  # Dimensionality of the latent space\n",
        "img_shape = 28*28  # Shape of the generated images (assuming MNIST-like images)\n",
        "\n",
        "# create instances of the Generator and Discriminator\n",
        "# generator = Generator(latent_dim, img_shape)\n",
        "# discriminator = Discriminator(img_shape)\n",
        "# Instantiate the Generator\n",
        "generator = Generator(latent_dim)\n",
        "\n",
        "# Instantiate the Discriminator\n",
        "discriminator = Discriminator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (l1): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=6272, bias=True)\n",
            "  )\n",
            "  (conv_blocks): Sequential(\n",
            "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Dropout2d(p=0.25, inplace=False)\n",
            "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (4): ZeroPad2d((0, 1, 0, 1))\n",
            "    (5): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (7): Dropout2d(p=0.25, inplace=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Dropout2d(p=0.25, inplace=False)\n",
            "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (15): Dropout2d(p=0.25, inplace=False)\n",
            "    (16): Flatten(start_dim=1, end_dim=-1)\n",
            "    (17): Linear(in_features=8192, out_features=1, bias=True)\n",
            "    (18): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Print out the models for inspection\n",
        "print(generator)\n",
        "print(discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC01vCuNaAlG"
      },
      "source": [
        "# Loss and optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like that, were done with the models, now lets set up the loss function and the optimizers for both the models.\n",
        "For the the timme being do not change them, the given models are pretty standard, and conventional for image gen via GANs.\n",
        "\n",
        "Other parameters however, you can change, and experiment with, such as the Epochs for example, feel free to experiment with these, and see what works best for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zgodGIc4XPRY"
      },
      "outputs": [],
      "source": [
        "# Loss and Optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpP_3JfBaAlG"
      },
      "source": [
        "# Training loop (5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have to load the data (preprocess it or do some image augmentation if you need) and train the model. \n",
        "\n",
        "Lets have brief look at how the loop for a vanilla GAN looks like, and then you can implement it yourself.\n",
        "\n",
        "Each epoch involves a couple of steps:\n",
        "1. Prep some data both fake and real\n",
        "2. Train the discriminator with some real data\n",
        "3. Train the discriminator with some fake data\n",
        "3. Train the generator\n",
        "\n",
        "The rest of its pretty similar to pytorch training loops, similar to the ones youve done in previous assingments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNkBGJnALBLV",
        "outputId": "188c9923-95e7-4611-f85a-9dc6a438c2c5"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m real_images \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m criterion(real_output, real_labels)\n\u001b[1;32m     16\u001b[0m d_loss_real\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m---> 66\u001b[0m     validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validity\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "# Training Loop\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(train_loader):\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Train Discriminator with real images\n",
        "        discriminator.zero_grad()\n",
        "        real_images = real_images.view(batch_size, -1)\n",
        "        real_output = discriminator(real_images)\n",
        "        d_loss_real = criterion(real_output, real_labels)\n",
        "        d_loss_real.backward()\n",
        "\n",
        "        # Train Discriminator with fake images\n",
        "        z = torch.randn(batch_size, latent_dim)\n",
        "        fake_images = generator(z)\n",
        "        fake_output = discriminator(fake_images.detach())\n",
        "        d_loss_fake = criterion(fake_output, fake_labels)\n",
        "        d_loss_fake.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        generator.zero_grad()\n",
        "        fake_output = discriminator(fake_images)\n",
        "        g_loss = criterion(fake_output, real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], \"\n",
        "                f\"Discriminator Loss: {(d_loss_real + d_loss_fake).item():.4f}, \"\n",
        "                f\"Generator Loss: {g_loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "# Save the trained models\n",
        "torch.save(generator.state_dict(), 'generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUpVr9nOaAlG"
      },
      "source": [
        "# Display images (2 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just display an image here using your generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ItnvNrYGXVDp",
        "outputId": "a76bdf1d-72c8-4a41-cfb2-fd54af49241f"
      },
      "outputs": [],
      "source": [
        "# Load the trained generator model\n",
        "generator = Generator(latent_dim, img_shape)\n",
        "generator.load_state_dict(torch.load('generator.pth'))\n",
        "generator.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Generate a random noise vector\n",
        "z = torch.randn(1, latent_dim)\n",
        "\n",
        "# Generate an image using the generator\n",
        "with torch.no_grad():\n",
        "    generated_image = generator(z).view(28, 28)  # Assuming MNIST image size\n",
        "\n",
        "# Convert the generated image tensor to a numpy array\n",
        "generated_image = generated_image.numpy()\n",
        "\n",
        "# Display the generated image\n",
        "plt.imshow(generated_image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh86GuU9aAlH"
      },
      "source": [
        "# Setting up latent space interpolation  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alright then, now that we are done with generating images, next lets take a look at the models latent space. \n",
        "\n",
        "What is it, and how do we use it?\n",
        "\n",
        "The latent space is the space of all possible inputs that the generator can take, and it is a vector space, and since it is a vector space we can do some vector operations on it, such as interpolation, which is what we will be doing here.\n",
        "\n",
        "To read up more on this, you can visit the following link: https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/ this is a pretty good article on the topic, which also talks about an implementation of this, we will be doing something similar here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So what you have to do here is generate two images of random numbers, once you have done this you have to interpolate between them, and display the images at each step of the interpolation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8B9FujaAlH"
      },
      "source": [
        "# Latent space time (5 marks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the images, lets interpolate between them, So what you have to do here is generate two images of random numbers, once you have done this you have to interpolate between them, and display the images at each step of the interpolation, the result here should be a series of images that show how the generator is transitioning from one image to another, how it goes from an image of 3 it generated to an image of 7 for example.\n",
        "\n",
        "Please make sure that there are atleast 10 steps including, the final and starting images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zawJThnfaAlH",
        "outputId": "9795a9c2-7565-4345-9606-2c6823052582"
      },
      "outputs": [],
      "source": [
        "def interpolate_points(p1, p2, n_steps=10):\n",
        "    # interpolate ratios between the points\n",
        "    ratios = torch.linspace(0, 1, steps=n_steps).unsqueeze(1)  # Add an extra dimension for broadcasting\n",
        "    # linear interpolate vectors\n",
        "    vectors = (1.0 - ratios) * p1 + ratios * p2\n",
        "    return vectors\n",
        "\n",
        "\n",
        "# Generate two random points in the latent space\n",
        "point1 = torch.randn(1, latent_dim)\n",
        "point2 = torch.randn(1, latent_dim)\n",
        "\n",
        "# Interpolate between them\n",
        "interpolated_points = interpolate_points(point1, point2)\n",
        "\n",
        "# Use the generator to create images from these interpolated points\n",
        "with torch.no_grad():\n",
        "    interpolated_images = generator(interpolated_points).view(-1, 28, 28).cpu().numpy()\n",
        "\n",
        "# Display the images\n",
        "fig, axs = plt.subplots(1, interpolated_images.shape[0], figsize=(15, 15))\n",
        "for i, img in enumerate(interpolated_images):\n",
        "    axs[i].imshow(img, cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector arithmatic in the latent space (5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, so if you were able to do that last part, one thing you may have noticed is that, in certain cases  when transitioning between one number and another, the image takes the form of some completely different number, for example, when transitioning from a 3 to a 6, the image may look like a 5 at some point, before going to a 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we interpolate in this latent space, we're essentially taking a path from the representation of one digit to another. However, because this space is learned from the data and is designed to capture the underlying variations within it, the path between any two points can pass through regions that represent a mix of features from both endpoints. \n",
        "\n",
        "This is equivalent to doing arithmetic in the latent space, and it's a powerful tool for manipulating the representations of the data. For example, if we take the representation of the number 1 and add the representation of the number 0, we can obtain a new representation that captures the shared features of both numbers. something that resembles the circular shape of the 0, but also the vertical line segment of the one so a 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is exactly what you have to do, this is what you have to do:\n",
        "\n",
        "1. Generate 2 images\n",
        "2. Perform additon on the latent space vectors of the images \n",
        "3. Display the results.\n",
        "\n",
        "do this with atleast 3 different pairs of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display two random images from the generator, then dispaly there additon.\n",
        "\n",
        "# Set the dimensionality of the latent space\n",
        "latent_dim = 100\n",
        "\n",
        "# Generate two random latent vectors\n",
        "z1 = torch.randn(1, latent_dim)\n",
        "z2 = torch.randn(1, latent_dim)\n",
        "\n",
        "# Generate images from these latent vectors\n",
        "with torch.no_grad():\n",
        "    img1 = generator(z1).view(28, 28).cpu().numpy()\n",
        "    img2 = generator(z2).view(28, 28).cpu().numpy()\n",
        "\n",
        "# Display the two original images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img1, cmap='gray')\n",
        "ax[0].set_title('Image 1')\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(img2, cmap='gray')\n",
        "ax[1].set_title('Image 2')\n",
        "ax[1].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Perform \"addition\" in the latent space by averaging the two latent vectors\n",
        "z_new = (z1 + z2) / 2\n",
        "\n",
        "# Generate a new image from the combined latent vector\n",
        "with torch.no_grad():\n",
        "    new_img = generator(z_new).view(28, 28).cpu().numpy()\n",
        "\n",
        "# Display the resulting image\n",
        "plt.imshow(new_img, cmap='gray')\n",
        "plt.title('Resulting Image')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display two random images from the generator, then dispaly there additon.\n",
        "# display two random images from the generator, then dispaly there additon.\n",
        "\n",
        "# Set the dimensionality of the latent space\n",
        "latent_dim = 100\n",
        "\n",
        "# Generate two random latent vectors\n",
        "z1 = torch.randn(1, latent_dim)\n",
        "z2 = torch.randn(1, latent_dim)\n",
        "\n",
        "# Generate images from these latent vectors\n",
        "with torch.no_grad():\n",
        "    img1 = generator(z1).view(28, 28).cpu().numpy()\n",
        "    img2 = generator(z2).view(28, 28).cpu().numpy()\n",
        "\n",
        "# Display the two original images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(img1, cmap='gray')\n",
        "ax[0].set_title('Image 1')\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(img2, cmap='gray')\n",
        "ax[1].set_title('Image 2')\n",
        "ax[1].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Perform \"addition\" in the latent space by averaging the two latent vectors\n",
        "z_new = (z1 + z2) / 2\n",
        "\n",
        "# Generate a new image from the combined latent vector\n",
        "with torch.no_grad():\n",
        "    new_img = generator(z_new).view(28, 28).cpu().numpy()\n",
        "\n",
        "# Display the resulting image\n",
        "plt.imshow(new_img, cmap='gray')\n",
        "plt.title('Resulting Image')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some Analytical questions (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q1) What would happen if you were to reduce the size of the dataset to 1/2 the number of images, and then train the GAN, how would the results differ, and why? Please do the following:\n",
        "\n",
        "1. Train the GAN with the 1/2 the dataset\n",
        "2. Generate images using this newly trained model\n",
        "3. Along with a couple of images give reasoning for the results, as well as a description of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ans here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q2) What would happen if you were to change the loss function of the GAN, how would the results differ, and why? Please do the following:\n",
        "\n",
        "1. Train the GAN with a different loss function, this could be any other loss function, it could be any of your choosing.\n",
        "2. Generate images using this newly trained model.\n",
        "3. Along with a couple of images give reasoning for the results, as well as a description of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ans here:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60bf3201337146fa80d609acf489e46a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd93097f2df41f291c594ef6e0daf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntSliderModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "step",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_60bf3201337146fa80d609acf489e46a",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_80b0792974074b8298938c4c2a16f45e",
            "value": 0
          }
        },
        "80b0792974074b8298938c4c2a16f45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "SliderStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "8babc5f63cea48caa809b10bae10ec76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aa94fcb58b64e7cb0de8815a42302e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fd93097f2df41f291c594ef6e0daf78",
              "IPY_MODEL_c00ccb2193974e11bfce3b0686ab9a29"
            ],
            "layout": "IPY_MODEL_d9e515badd1d4107a6d039d04a9388f5"
          }
        },
        "c00ccb2193974e11bfce3b0686ab9a29": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8babc5f63cea48caa809b10bae10ec76",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "image/png": "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\n",
                  "text/plain": "<Figure size 640x480 with 1 Axes>"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "d9e515badd1d4107a6d039d04a9388f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
